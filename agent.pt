
#         # Installations 
#     !pip install --upgrade pip
#     !pip install ollama
#     !pip install langchain
#     !pip install langchain_community
#     !pip install -U langchain-ollama
#     !pip install typing_extensions==4.7.1
#     !pip install typing_extensions --upgrade
#     !pip install httpx=='0.23.3'
#     !pip install fastembed
#     !pip install phi
#     !pip install faiss-cpu
#     !pip install sentence_transformers





from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from sentence_transformers import SentenceTransformer
import numpy as np
import os
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
import time
import faiss

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
# from langchain_qdrant import QdrantVectorStore

import requests
import xml.etree.ElementTree as ET
import re
import html
from langchain_community.document_loaders import WebBaseLoader
import json

from rich.console import Console
from rich.panel import Panel
from rich.json import JSON


import re
import html

def rag_url_navigation(model,model_llm,user_query):
        
    def get_relevant_chunks(file_path, query, k=3):
        with open(file_path, 'r') as f:
            full_text = f.read()

        # Chunk using langchain splitter
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        chunks = splitter.split_text(full_text)

        # Embed chunks using sentence-transformers
        chunk_embeddings = model.encode(chunks)

        # Build FAISS index manually
        dim = chunk_embeddings[0].shape[0]
        index = faiss.IndexFlatL2(dim)
        index.add(np.array(chunk_embeddings))

        # Query embedding
        query_emb = model.encode([query])
        _, I = index.search(query_emb, k)

        return [chunks[i] for i in I[0]]
    
    
    
    

    def find_best_url(query):
        query_emb = model.encode([query])[0]
        scores = np.dot(url_embeddings_keywords, query_emb)
    #     print("The best URL finding Score is",scores)
    #     print()
        c = -1
        list_score = list(scores)
        print(list_score)

        for k in url_to_file.keys():
            c = c+1
            print("URL : ",k,"Score : ",list_score[c])
            print()

        best_idx = int(np.argmax(scores))
        return urls[best_idx]

    # Step 2: Load & chunk
    def get_relevant_chunks_old(file_path, query):
        with open(file_path, 'r') as f:
            full_text = f.read()

        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        docs = splitter.create_documents([full_text])

        # Embed and create in-memory FAISS
        faiss_index = FAISS.from_documents(docs, embedding_wrapper)
        relevant_docs = faiss_index.similarity_search(query, k=3)

        return "\n".join([doc.page_content for doc in relevant_docs])

    
    

    def call_llm(model_llm,user_query):
        system_prompt = """ "You are a helpful AI bot with extensive domain knowledge of Straight Talk website details, 
        an internet and telecommunications company. Respond the the user queries and perform the requests as asked."""


        prompt = ChatPromptTemplate([
            ("system",system_prompt),
            ("human", "{user_question}")
        ])

        chain = prompt | model_llm

        start_time = time.time()

        response = chain.invoke({"user_question": user_query})

        end_time = time.time()

        execution_time = end_time - start_time

        print(f"Execution time: {execution_time} seconds \n")
        print()
        print()

        print("Model Response below")
        print()

        print(response)
        return response
        
        
   

    def rag_pipeline(query):
        best_url = find_best_url(query)
        file_path = url_to_file[best_url]
        context = get_relevant_chunks(file_path[0], query)
        print("User Query:",query)
        print()
        print("File Matched with User Query",file_path)
        print()
        print("Processing....")
        print()
        print()
        print("Taking Relavent Chunks")
        print()
        print()
        print("Chunk Taken: ")
        print()
        print(context)
        print()
        print()
        print()

        final_prompt = f"Context:\n{context}\n\nQuestion:\n{query}\n\nAnswer:"
        # Send to LLM of your choice here

        print("Calling LLM Model...")
        response_llm = call_llm(model_llm,final_prompt)
        return response_llm


        
    keywords_terms_conditions = """Agreement to Arbitrate Disputes
    Express Written Consent to Receive Communications
    ITerms and Conditions of Service
    International Long Distance ServiceHome Phone
    Home Internet
    VMobile Hotspots and Bring Your Own Tablet
    Return Policy return eligible for return walmart purchase online straighttalk purchase limited warranty terms of limited warranty 
    Limited WarrantyUnlocking Policy
    """

    keywords_tradein = """
    tradein opt for trade in upgrade and get paid trade in old device old device exchange trade in program
    """
    base_path = '/mnt/Agents AI Model/Rag/Updated_code/Chat_with_PDF/CrawlAIWith_langchain/Using_JinaAI/corpus_extracted_jina/'

    url_to_file = {'https://www.straighttalk.com/nascar/contact':[f'{base_path}Contact Us - Customer & Media Contact Information - Straight Talk_.txt','contact us cusomer media support how to contact'],
                    'https://www.straighttalk.com/privacy-policy':[f'{base_path}Privacy Policy_.txt','policy personal information privacy information  secure retain personal information childern blogs refer a friend right to know delete access information ccpa request reporting  other california privacy rights  submit privacy request  nevada privacy rights oreggon region changes to policy  recent changes contact privacy'],

                   'https://www.straighttalk.com/support/terms-conditions':[f'{base_path}Arbitration, Consent & Policies - Straight Talk Terms & Conditions_.txt',keywords_terms_conditions],
                   'https://www.straighttalk.com/more-options/trade-in-program':[f'{base_path}Get Paid When You Trade In Phone | Straight Talk Wireless_.txt',keywords_tradein]

    }

    
    # Step 1: Create initial URL retrieval index
    url_texts = []
    urls = []
    url_keys = []
    for url, file_path in url_to_file.items():
        with open(file_path[0], 'r') as f:
            content = f.read()  # First 500 characters as summary
        keywords_url = file_path[1]
        url_texts.append(content)
        url_keys.append(keywords_url)
        urls.append(url)

   
    url_embeddings = model.encode(url_texts)
    url_embeddings_keywords = model.encode(url_keys)
#     len(url_keys)
    
    model_final_response = rag_pipeline(user_query)
    return model_final_response

    


